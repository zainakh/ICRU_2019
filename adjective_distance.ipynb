{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    RT @KenRenar96: Open your eyes you vote for Hi...\n",
       "1    SEE VIDEO! Hillary Clinton Plays Who'd You Rat...\n",
       "2    RT @RealFKNNews: #US #media to #prematurely #P...\n",
       "3    SEE VIDEO! Who Is Hillary Clinton?... https://...\n",
       "4    Hillary Clinton #LoveTrumpsHate Large Black Cu...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data frame\n",
    "df = pd.read_csv('/media/zainkhan/USB30FD/suspended-clinton-tweets.txt', encoding='latin1', error_bad_lines=False, warn_bad_lines=False, header=None)\n",
    "df = df[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [rt, @kenrenar96:, open, eyes, vote, hillary, ...\n",
       "1    [see, video!, hillary, clinton, plays, who'd, ...\n",
       "2    [rt, @realfknnews:, #us, #media, #prematurely,...\n",
       "3    [see, video!, hillary, clinton?..., https://t....\n",
       "4    [hillary, clinton, #lovetrumpshate, large, bla...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words\n",
    "df = df.str.lower().str.split()\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "df = df.apply(lambda x: [item for item in x if item not in stop])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rt', '@kenrenar96:', 'open', 'eyes', 'vote', 'hillary', 'clinton.', 'salvation.', 'believes', 'ideals', 'equality!!!', '#hillaryforpr'], ['see', 'video!', 'hillary', 'clinton', 'plays', \"who'd\", 'rather?...', 'https://t.co/aho7bx9vwp', 'https://t.co/6egopos352']]\n"
     ]
    }
   ],
   "source": [
    "# Separate into sentences\n",
    "sentences = df.tolist()\n",
    "print(sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['open', 'eyes', 'vote', 'hillary', 'clinton', 'salvation', 'believes', 'ideals', 'equality'], ['video', 'hillary', 'clinton', 'plays', 'whod', 'rather']]\n"
     ]
    }
   ],
   "source": [
    "# Clean and remove illegal characters\n",
    "import re\n",
    "def clean(s):\n",
    "    for i, w in enumerate(s):\n",
    "        if contains_illegal(w):\n",
    "            s[i] = ''\n",
    "        else:\n",
    "            if 'cli' in w:\n",
    "                s[i] = 'clinton'\n",
    "            elif 'hrc' in w or 'hil' in w:\n",
    "                s[i] = 'hillary'\n",
    "            elif 'tru' in w or 'trmp' in w:\n",
    "                s[i] = 'trump'\n",
    "            elif 'bern' in w:\n",
    "                s[i] = 'bernie'\n",
    "            elif 'sander' in w:\n",
    "                s[i] = 'sanders'\n",
    "            extras = '0123456789~_=*^,.-`%\"\"+#&\"!?<>:;/\\\\\\'()[]{}$|\\x91\\x92\\x93\\x94\\x96\\x97'\n",
    "            s[i] = s[i].translate({ord(c):'' for c in extras})\n",
    "            if len(s[i]) < 4:\n",
    "                s[i] = ''\n",
    "    return s\n",
    "                    \n",
    "def contains_illegal(w):\n",
    "    illegal = ['@', '#', 'htt', 'via', 'desd']\n",
    "    if any(x in w for x in illegal):\n",
    "        return True\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", w) is not None:\n",
    "        return True\n",
    "    return False \n",
    "\n",
    "sentences = [clean(s) for s in sentences if len(s) > 0]\n",
    "sentences = [list(filter(None, sentence)) for sentence in sentences]\n",
    "print(sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accessible', 'active', 'adaptable', 'admirable', 'adventurous']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get adjective list to iterate over\n",
    "adj = pd.read_csv('adjectives-character-richards.csv')\n",
    "adj = adj.values.tolist()\n",
    "adj = [word[0] for word in adj]\n",
    "adj[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['trump', 'says', 'hillary', 'clinton', 'getting', 'pumped', 'debate', 'says', 'take', 'drug', 'test'], ['difference', 'hillary', 'sanders', 'attacked', 'clinton', 'record', 'clinton', 'attacked', 'bernie', 'fans']]\n"
     ]
    }
   ],
   "source": [
    "# Make sentences unique\n",
    "unique_sentences = [list(x) for x in set(tuple(x) for x in sentences)]\n",
    "print(unique_sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps adjective occurences per sentence relative to a word\n",
    "def adj_to_word(sentences, adj_list, target):\n",
    "    adj_to_word = {}\n",
    "    sentences_with_target = [s for s in sentences if target in s]\n",
    "\n",
    "    for adj in adj_list:\n",
    "        for sentence in sentences_with_target:\n",
    "            if adj in sentence:\n",
    "                if adj in adj_to_word:\n",
    "                    adj_to_word[adj] += 1\n",
    "                else:\n",
    "                    adj_to_word[adj] = 1\n",
    "    return adj_to_word\n",
    "\n",
    "target_word = 'trump'\n",
    "d = adj_to_word(sentences, adj, target_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['intuitive', 994], ['honest', 822], ['criminal', 616]]\n"
     ]
    }
   ],
   "source": [
    "dlist = []\n",
    "for key, value in d.items():\n",
    "    temp = [key,value]\n",
    "    dlist.append(temp)\n",
    "dlist.sort(key=lambda x: x[1], reverse=True)\n",
    "print(dlist[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump\n",
      "['intuitive', 994]\n",
      "['honest', 822]\n",
      "['criminal', 616]\n",
      "['brutal', 347]\n",
      "['false', 338]\n",
      "['political', 317]\n",
      "['kind', 305]\n",
      "['winning', 279]\n",
      "['open', 240]\n",
      "['private', 218]\n",
      "['liberal', 207]\n",
      "['sane', 197]\n",
      "['strong', 197]\n",
      "['dirty', 160]\n",
      "['stupid', 141]\n",
      "['critical', 131]\n",
      "['weak', 122]\n",
      "['cold', 119]\n",
      "['physical', 115]\n",
      "['obvious', 106]\n",
      "['dull', 102]\n",
      "['tough', 101]\n",
      "['angry', 100]\n",
      "['responsible', 93]\n",
      "['attractive', 92]\n",
      "['clean', 87]\n",
      "['conservative', 87]\n",
      "['greedy', 84]\n",
      "['fraudulent', 82]\n",
      "['simple', 79]\n",
      "['disturbing', 76]\n",
      "['serious', 75]\n",
      "['brilliant', 74]\n",
      "['desperate', 71]\n",
      "['proud', 69]\n",
      "['insulting', 69]\n",
      "['loyal', 68]\n",
      "['crazy', 66]\n",
      "['confused', 64]\n",
      "['deep', 59]\n",
      "['intense', 58]\n",
      "['extreme', 54]\n",
      "['dishonest', 53]\n",
      "['independent', 51]\n",
      "['wise', 51]\n",
      "['hostile', 50]\n",
      "['formal', 48]\n",
      "['grand', 48]\n",
      "['clever', 47]\n",
      "['fiery', 47]\n",
      "['focused', 43]\n",
      "['secure', 41]\n",
      "['original', 40]\n",
      "['modern', 40]\n",
      "['deceitful', 40]\n",
      "['alert', 39]\n",
      "['decisive', 39]\n",
      "['impressive', 38]\n",
      "['crude', 33]\n",
      "['narrow', 33]\n",
      "['solid', 32]\n",
      "['fair', 31]\n",
      "['popular', 31]\n",
      "['questioning', 29]\n",
      "['exciting', 28]\n",
      "['unstable', 28]\n",
      "['bizarre', 27]\n",
      "['surprising', 26]\n",
      "['constant', 25]\n",
      "['progressive', 25]\n",
      "['ridiculous', 25]\n",
      "['forgiving', 24]\n",
      "['religious', 21]\n",
      "['sharing', 19]\n",
      "['friendly', 18]\n",
      "['vulnerable', 18]\n",
      "['earnest', 17]\n",
      "['knowledge', 17]\n",
      "['reserved', 17]\n",
      "['vague', 17]\n",
      "['healthy', 16]\n",
      "['hateful', 16]\n",
      "['steady', 15]\n",
      "['understanding', 15]\n",
      "['sexy', 14]\n",
      "['soft', 14]\n",
      "['provocative', 14]\n",
      "['dominating', 13]\n",
      "['pure', 13]\n",
      "['ignorant', 13]\n",
      "['insecure', 13]\n",
      "['calm', 12]\n",
      "['helpful', 12]\n",
      "['confident', 11]\n",
      "['decent', 11]\n",
      "['stable', 11]\n",
      "['careless', 11]\n",
      "['frightening', 11]\n",
      "['uncaring', 11]\n",
      "['capable', 10]\n",
      "['moderate', 10]\n",
      "['neat', 9]\n",
      "['subtle', 9]\n",
      "['aggressive', 9]\n",
      "['determined', 9]\n",
      "['irresponsible', 9]\n",
      "['artful', 8]\n",
      "['organized', 7]\n",
      "['patriotic', 7]\n",
      "['emotional', 7]\n",
      "['fixed', 7]\n",
      "['paranoid', 7]\n",
      "['transparent', 7]\n",
      "['educated', 6]\n",
      "['enthusiastic', 6]\n",
      "['firm', 6]\n",
      "['peaceful', 6]\n",
      "['sympathetic', 6]\n",
      "['amoral', 6]\n",
      "['demanding', 6]\n",
      "['difficult', 6]\n",
      "['foolish', 6]\n",
      "['lazy', 6]\n",
      "['naive', 6]\n",
      "['caring', 5]\n",
      "['curious', 5]\n",
      "['extraordinary', 5]\n",
      "['passionate', 5]\n",
      "['polished', 5]\n",
      "['shrewd', 5]\n",
      "['complex', 5]\n",
      "['sarcastic', 5]\n",
      "['cautious', 5]\n",
      "['ruined', 5]\n",
      "['sadistic', 5]\n",
      "['silly', 5]\n",
      "['challenging', 4]\n",
      "['daring', 4]\n",
      "['gracious', 4]\n",
      "['logical', 4]\n",
      "['principled', 4]\n",
      "['warm', 4]\n",
      "['busy', 4]\n",
      "['deceptive', 4]\n",
      "['driving', 4]\n",
      "['predictable', 4]\n",
      "['quiet', 4]\n",
      "['compulsive', 4]\n",
      "['obnoxious', 4]\n",
      "['outrageous', 4]\n",
      "['prim', 4]\n",
      "['slow', 4]\n",
      "['unhealthy', 4]\n",
      "['articulate', 3]\n",
      "['charming', 3]\n",
      "['dedicated', 3]\n",
      "['disciplined', 3]\n",
      "['dramatic', 3]\n",
      "['hardworking', 3]\n",
      "['sensitive', 3]\n",
      "['ordinary', 3]\n",
      "['outspoken', 3]\n",
      "['cruel', 3]\n",
      "['disrespectful', 3]\n",
      "['fearful', 3]\n",
      "['gullible', 3]\n",
      "['shallow', 3]\n",
      "['trendy', 3]\n",
      "['unconvincing', 3]\n",
      "['accessible', 2]\n",
      "['charismatic', 2]\n",
      "['directed', 2]\n",
      "['dynamic', 2]\n",
      "['energetic', 2]\n",
      "['generous', 2]\n",
      "['honorable', 2]\n",
      "['magnanimous', 2]\n",
      "['objective', 2]\n",
      "['profound', 2]\n",
      "['rational', 2]\n",
      "['realistic', 2]\n",
      "['sweet', 2]\n",
      "['amusing', 2]\n",
      "['casual', 2]\n",
      "['cute', 2]\n",
      "['neutral', 2]\n",
      "['stern', 2]\n",
      "['blunt', 2]\n",
      "['dependent', 2]\n",
      "['misguided', 2]\n",
      "['narcissistic', 2]\n",
      "['petty', 2]\n",
      "['predatory', 2]\n",
      "['rowdy', 2]\n",
      "['active', 1]\n",
      "['appreciative', 1]\n",
      "['aspiring', 1]\n",
      "['balanced', 1]\n",
      "['conscientious', 1]\n",
      "['courageous', 1]\n",
      "['eloquent', 1]\n",
      "['faithful', 1]\n",
      "['insightful', 1]\n",
      "['intelligent', 1]\n",
      "['methodical', 1]\n",
      "['optimistic', 1]\n",
      "['precise', 1]\n",
      "['relaxed', 1]\n",
      "['reliable', 1]\n",
      "['sentimental', 1]\n",
      "['sporting', 1]\n",
      "['contradictory', 1]\n",
      "['restrained', 1]\n",
      "['skeptical', 1]\n",
      "['smooth', 1]\n",
      "['agonizing', 1]\n",
      "['calculating', 1]\n",
      "['coarse', 1]\n",
      "['complacent', 1]\n",
      "['cowardly', 1]\n",
      "['crass', 1]\n",
      "['devious', 1]\n",
      "['erratic', 1]\n",
      "['indecisive', 1]\n",
      "['insensitive', 1]\n",
      "['moody', 1]\n",
      "['natty', 1]\n",
      "['perverse', 1]\n",
      "['pretentious', 1]\n",
      "['rigid', 1]\n",
      "['sanctimonious', 1]\n",
      "['secretive', 1]\n",
      "['sordid', 1]\n",
      "['suspicious', 1]\n",
      "['tasteless', 1]\n",
      "['tense', 1]\n",
      "['troublesome', 1]\n",
      "['undisciplined', 1]\n",
      "['willful', 1]\n"
     ]
    }
   ],
   "source": [
    "print(target_word)\n",
    "for i in dlist:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
