{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    RT @KenRenar96: Open your eyes you vote for Hi...\n",
       "1    SEE VIDEO! Hillary Clinton Plays Who'd You Rat...\n",
       "2    RT @RealFKNNews: #US #media to #prematurely #P...\n",
       "3    SEE VIDEO! Who Is Hillary Clinton?... https://...\n",
       "4    Hillary Clinton #LoveTrumpsHate Large Black Cu...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data frame\n",
    "df = pd.read_csv('/media/zainkhan/USB30FD/suspended-clinton-tweets.txt', encoding='latin1', error_bad_lines=False, warn_bad_lines=False, header=None)\n",
    "df = df[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "df = df.str.lower().str.split()\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "df = df.apply(lambda x: [item for item in x if item not in stop])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Stem dataframe\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "def stem_sentences(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "df = df.apply(stem_sentences)\n",
    "df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into sentences\n",
    "sentences = df.tolist()\n",
    "print(sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and remove illegal characters\n",
    "import re\n",
    "def clean(s):\n",
    "    for i, w in enumerate(s):\n",
    "        if contains_illegal(w):\n",
    "            s[i] = ''\n",
    "        else:\n",
    "            if 'cli' in w:\n",
    "                s[i] = 'clinton'\n",
    "            elif 'hrc' in w or 'hil' in w:\n",
    "                s[i] = 'hillary'\n",
    "            elif 'tru' in w or 'trmp' in w:\n",
    "                s[i] = 'trump'\n",
    "            elif 'bern' in w:\n",
    "                s[i] = 'bernie'\n",
    "            extras = '0123456789~_=*^,.-`%\"\"+#&\"!?<>:;/\\\\\\'()[]{}$|\\x91\\x92\\x93\\x94\\x96\\x97'\n",
    "            s[i] = s[i].translate({ord(c):'' for c in extras})\n",
    "            if len(s[i]) < 4:\n",
    "                s[i] = ''\n",
    "    return s\n",
    "                    \n",
    "def contains_illegal(w):\n",
    "    illegal = ['@', '#', 'htt', 'via', 'desd']\n",
    "    if any(x in w for x in illegal):\n",
    "        return True\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", w) is not None:\n",
    "        return True\n",
    "    return False \n",
    "\n",
    "sentences = [clean(s) for s in sentences if len(s) > 0]\n",
    "sentences = [list(filter(None, sentence)) for sentence in sentences]\n",
    "print(sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences, size=100, window=5, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of terms, indices, term counts from model \n",
    "ordered_vocab = [(term, voc.index, voc.count) for term, voc in model.wv.vocab.items()]\n",
    "\n",
    "# Sort by term counts so common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda k: -k[2])\n",
    "\n",
    "# Unzip terms, indices, and counts\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = pd.DataFrame(model.wv.vectors[term_indices,:], index=ordered_terms)\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def related_terms(model, token, topn=10):\n",
    "    out = []\n",
    "    for word, similarity in model.wv.most_similar(positive=[token], topn=topn):\n",
    "        out.append([word, round(similarity, 3)])\n",
    "    return out\n",
    "\n",
    "for elem in related_terms(model, 'hillary'):\n",
    "    print(elem)\n",
    "print('\\n')\n",
    "for elem in related_terms(model, 'clinton'):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(model, token, topn=10):\n",
    "    base = related_terms(model, token, topn)\n",
    "    network = []\n",
    "    network.append([[i] for i in base])\n",
    "    for i in range(topn, 0, -1):\n",
    "        new_related = related_terms(model, base[topn - i][0], i)\n",
    "        network.append([new_related, [0], [topn - i + 1]])\n",
    "        \n",
    "        for layer in range(i//2, 0, -1):\n",
    "            layered_related = related_terms(model, new_related[i//2 - layer][0], layer)\n",
    "            network.append([layered_related, [i//2 - layer + 1], [topn - i + 1]])\n",
    "    return network\n",
    "\n",
    "center_word = 'clinton'\n",
    "word_network = network(model, center_word)\n",
    "print('Model centered around word ' + center_word)\n",
    "for words in word_network:\n",
    "    for items in words: \n",
    "        for word in items:\n",
    "            print(word)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-LU9BwZWI",
   "language": "python",
   "name": "research-lu9bwzwi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
